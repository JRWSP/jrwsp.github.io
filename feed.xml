<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jrwsp.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jrwsp.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-29T15:20:47+00:00</updated><id>https://jrwsp.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Exploring the MLX</title><link href="https://jrwsp.github.io/blog/2025/MLX/" rel="alternate" type="text/html" title="Exploring the MLX"/><published>2025-04-24T16:40:16+00:00</published><updated>2025-04-24T16:40:16+00:00</updated><id>https://jrwsp.github.io/blog/2025/MLX</id><content type="html" xml:base="https://jrwsp.github.io/blog/2025/MLX/"><![CDATA[<p>-เนื้อหาเผยแพร่ครั้งแรกที่<a href="https://paragraph.com/@jisaiqq.eth/exploring-the-mlx-an-optimized-matrix-multiplication-on-apple-silicon">paragraph</a>-</p> <p>หลังจากตรากตรำกับการเขียนงานวิจัยมาหลายเดือน ในที่สุดก็มีเวลาว่างได้พักกายพักใจซักที</p> <p>พอหัวมันโล่งก็พอคิดไปถึงเรื่องหลายๆอย่างที่อยากทำ หนึ่งในนั้นคืออยากลองเล่นเจ้า<a href="https://github.com/ml-explore/mlx">MLX</a> ที่appleเปิดตัวมาได้ซักพักใหญ่ๆ ที่เป็นarray frameworkสำหรับapple siliconโดยเฉพาะ ถ้าใครนึกภาพไม่ออกมันคือNumPyที่optimizeมาสำหรับเครื่องMacนั่นแหละ โดยข้อดีหลักๆของเจ้าMLXอ้างอิงจากหน้าgithubเค้าบอกว่า</p> <ul> <li>APIเหมือนNumPyหรือPyTorchเลย ถ้าใครคุ้นเคยกับสองตัวนี้อยู่แล้วน่าจะปรับตัวได้ไม่ยาก</li> <li>ใช้ประโยชน์จากunified memoryได้อย่างเต็มที่ ข้อดีทำให้การคำนวณด้วยgpuไม่มีoverheadsจากการคักลอกข้อมูลขึ้นไปที่ramของgpu และโค็ดที่สั่งคำนวณบนcpuหรือgpuก็ทำได้ค่อนข้างตรงไปตรงมาโดยไม่ต้องมีการcopyตัวแปร ดังตัวอย่างด้านล่าง</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">mx</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">((</span><span class="mi">100</span><span class="p">,))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">mx</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">((</span><span class="mi">100</span><span class="p">,))</span>
<span class="n">mx</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">mx</span><span class="p">.</span><span class="n">cpu</span><span class="p">)</span>
<span class="n">mx</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">mx</span><span class="p">.</span><span class="n">gpu</span><span class="p">)</span>
</code></pre></div></div> <h1 id="แล้วmlxมันดีกว่าpytorchหรือปล่าวนะ">แล้วMLXมันดีกว่าPyTorchหรือปล่าวนะ?</h1> <p>ในฐานะที่ใช้NumPyทำงานมาตลอดเพราะdataที่ไม่ใหญ่มากนั้นcpuยังทำงานได้เร็วพอ นานๆทีจะมีdataใหญ่มากพอให้ต้องการการคำนวณบนGPU ซึ่งกรณีนี้ก็มักจะใช้บริการPyTorchซึ่งมี Metal Performance Shaders (MPS) backendสำหรับเรียกใช้gpuบนmacได้อยู่แล้ว ด้วยความที่PyTorchได้รับความนิยมในวงกว้างมากกว่าในงานAI,ML รวมถึงซัพพอร์ทOSและdeviceที่เยอะกว่า อยู่ๆจะหันไปใช้MLXที่เฉพาะเจาะจงกว่าทำไมถ้ามันไม่เร็วกว่า จริงไหม?</p> <p>ดังนั้นจึงต้องทดสอบ</p> <p>ว่าแล้วก็ลงมือโหลดmlxมาลงแล้วเขียนbenchmarkซะเลย โดยการทดสอบจะทำโดยการจับเวลาการคูณเมทริคที่สุ่มมาไม่ซ้ำกัน10ครั้งแล้วใช้เวลาเฉลี่ย โดยคูณด้วยbackendsต่างๆดังนี้</p> <ol> <li>NumPy (cpu)</li> <li>Torch (cpu)</li> <li>Torch (gpu by mps)</li> <li>MLX (cpu)</li> <li>MLX (gpu)</li> </ol> <p>การคำนวณทั้งหมดใช้ตัวแปรความประเภทfloat32</p> <p>สเปคเครื่องที่ใช้ทดสอบ Macbook Air M1 2020 (16GB) 7-cores gpu.</p> <p>Package version: python3.13.3, mlx v0.25.0, pytorch v2.5.1, numpy 2.2.5</p> <h1 id="ผลลัพธ์">ผลลัพธ์:</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-04-24-MLX/1-480.webp 480w,/assets/img/posts/2025-04-24-MLX/1-800.webp 800w,/assets/img/posts/2025-04-24-MLX/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-04-24-MLX/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-04-24-MLX/2-480.webp 480w,/assets/img/posts/2025-04-24-MLX/2-800.webp 800w,/assets/img/posts/2025-04-24-MLX/2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-04-24-MLX/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>จากการทดสอบถือว่าน่าประทับใจทีเดียวสำหรับmlx โดยประสิทธิภาพของการใช้gpuระหว่างMLX(gpu)เร็วกว่าบนTorch(mps)หรือใกล้เคียง ซึ่งน่าจะเป็นผลมาจากการใช้unified memoryช่วยประหยัดเวลาจากการส่งข้อมูลขึ้นแรมgpuนั่นเอง</p> <p>ที่น่าประหลาดใจคือความเร็วของ MLX(cpu)มีspeedupดีกว่าTorch(cpu)กับNumPyอย่างชัดเจน(สองอย่างหลังแทบไม่ต่างกัน) แถมบางกรณียังเทียบเคียงgpuได้เลยทีเดียว ซึ่งจากการตรวจสอบคาดว่าสาเหตุน่าจะมาจากMLX(cpu)มีการใช้งานefficient coreในการคำนวณด้วย ในขณะที่NumPyกับTorch(cpu)ใช้แค่Performance coreเท่านั้น</p> <p>สรุป: ประสิทธิภาพของMLXสำหรับการคูณเมทริคถือว่าน่าประทับใจมาก โดยหลักๆมาจากการใช้ประโยชน์ของunified memoryและการใช้งานcpuทั้งP-coresและE-cores</p> <p>ดังนั้น MLXจึงเป็นตัวเลือกที่น่าสนใจอย่างมากสำหรับงานคำนวณบนฮาร์ดแวร์ที่ใช้apple silicon</p>]]></content><author><name></name></author><category term="posts"/><category term="python"/><summary type="html"><![CDATA[An Optimized Matrix Multiplication on Apple Silicon]]></summary></entry></feed>